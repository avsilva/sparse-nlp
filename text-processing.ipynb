{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T23:01:20.444672Z",
     "start_time": "2018-04-05T23:01:20.267548Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os, json, sys\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from PIL import Image\n",
    "#from time import time\n",
    "import utils.database as db\n",
    "import utils.corpora as corp\n",
    "import utils.fingerprints as finger\n",
    "\n",
    "import operator\n",
    "from scipy.sparse import csr_matrix\n",
    "from sparse_som import *\n",
    "import numpy as np\n",
    "import re\n",
    "import conf.conn as cfg\n",
    "#from tqdm import tnrange, tqdm_notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Insert snippets into database</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T23:02:12.292863Z",
     "start_time": "2018-04-05T23:02:12.030533Z"
    },
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: <class 'FileNotFoundError'>\n"
     ]
    }
   ],
   "source": [
    "#set wikipedia folder where dump json files are located\n",
    "try:\n",
    "    wikifilesdir = \"F:/RESEARCH/TESE/corpora/wikifiles/01012018/json\"\n",
    "    os.chdir(wikifilesdir)\n",
    "except:\n",
    "    print('ERROR: ' +str(sys.exc_info()[0]))\n",
    "\n",
    "#register file system json files in DB\n",
    "def register():\n",
    "    i = 0\n",
    "    for subdir, dirs, files in os.walk(wikifilesdir):\n",
    "    \n",
    "        for file in files:\n",
    "            if i % 5 == 0:\n",
    "                print (str(i)+ ' files processed')\n",
    "            #print (os.path.join(subdir, file))\n",
    "            filepath = subdir + os.sep + file\n",
    "            path = filepath.split(os.sep)[-3:]\n",
    "            path = '/'.join(path)\n",
    "            db.register_file(cfg, path)\n",
    "            i = i + 1\n",
    "\n",
    "    print (\"DONE. %s files registered \"% (i))\n",
    "\n",
    "#process file system json files in DB and move file to processed destination folder\n",
    "def process():\n",
    "    limit = 100\n",
    "    i = 0\n",
    "    files = db.select_files(cfg, limit)\n",
    "    processed_dir = wikifilesdir+'-processed'\n",
    "    for file in files:\n",
    "        if i % 5 == 0:\n",
    "            print (str(i)+ ' files processed')\n",
    "        path = file[1]\n",
    "        file_id = file[0]\n",
    "        filepath = wikifilesdir+'/'+path\n",
    "        with open(filepath, 'r', encoding='utf-8') as fh:\n",
    "            datafile=fh.readlines()\n",
    "            db.process_file(cfg, datafile, file_id)\n",
    "        fh.close();\n",
    "        move_file(path, wikifilesdir, processed_dir)\n",
    "        i = i + 1\n",
    "        \n",
    "def get_tables_sizes():\n",
    "    table_sizes = db.get_table_size(cfg)\n",
    "    return table_sizes\n",
    "\n",
    "def update_stats():\n",
    "    snippets_size = ''\n",
    "    engine = create_engine('postgresql://postgres@localhost:5432/sparsenlp')\n",
    "    sql = \"select b.id, b.cleaned_text as text from snippets b \"\n",
    "    sql += \"where cleaned = 't'\"\n",
    "    data = pd.read_sql_query(sql, con=engine)\n",
    "    #print (data.shape)\n",
    "    word_counts = corp.get_word_counts_per_snippet(data, clean_text=False)\n",
    "    freqs = corp.get_frequencies(word_counts)\n",
    "    print (freqs['1960'])\n",
    "    print (sorted(freqs.items(), key=operator.itemgetter(1), reverse=True)[:20])\n",
    "    print (word_counts[5])\n",
    "    vocabulary = corp.get_vocabulary(word_counts)\n",
    "    print(\"O vocabulário tem %d palavras\"% len(vocabulary) )\n",
    "    table_sizes = get_tables_sizes()\n",
    "    for row in table_sizes:\n",
    "        if row[0] == 'snippets':\n",
    "            snippets_size = row[1]\n",
    "    db.update_words_vs_snippets(cfg, data.shape[0], len(vocabulary), snippets_size)\n",
    "    \n",
    "def process_snippets(_limit):\n",
    "    #tokenizer for snippets \n",
    "    #500 snippets -> 30 min\n",
    "    engine = create_engine('postgresql://postgres@localhost:5432/sparsenlp')\n",
    "    sql = \"select id, text from snippets\"\n",
    "    sql += \"where length(text) > 100 and cleaned = 'f' and random() < 0.01 limit \"+str(_limit)\n",
    "    data = pd.read_sql_query(sql, con=engine)\n",
    "    #print (data.shape)\n",
    "    cleaned_texts = corp.clean_text(data, remove_stop_words=True, remove_punct=True, \n",
    "                   lemmas=True, remove_numbers=True, remove_spaces=True, remove_2letters_words=True, \n",
    "                   remove_apostrophe=True, method='spacy', spacy_disabled_components=['tagger', 'parser'])\n",
    "    for text in cleaned_texts:\n",
    "        print (text['id'])\n",
    "        db.insert_cleaned_text(cfg, text['id'], text['snippet'])\n",
    "        \n",
    "def get_cleaned_data():\n",
    "    conn_string = 'postgresql://postgres@localhost:5432/sparsenlp'\n",
    "    engine = create_engine(conn_string)\n",
    "    sql = \"select id, cleaned_text as text from snippets where cleaned = 't'\"\n",
    "    dataframe = pd.read_sql_query(sql, con=engine)\n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T23:02:13.620514Z",
     "start_time": "2018-04-05T23:02:13.422327Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('files', '144 kB'),\n",
       " ('articles', '2264 kB'),\n",
       " ('news', '51 MB'),\n",
       " ('snippets', '576 MB'),\n",
       " ('words_vs_snippets', '8192 bytes'),\n",
       " ('chunks', '8192 bytes')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#register()\n",
    "#process()\n",
    "get_tables_sizes()\n",
    "#process_snippets(10)\n",
    "#update_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-25T22:42:58.443555Z",
     "start_time": "2018-03-25T22:42:53.731554Z"
    },
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'id': 987738, 'definition': 2, 'truck': 2, 'universally': 1, 'accepted': 1, 'suv': 1, 'government': 1, 'regulation': 1, 'simply': 1, 'category': 1, 'highway': 1, 'vehicle': 1, 'turn': 1, 'lump': 1, 'pickup': 1, 'minivan': 1, 'light': 1, 'auto': 1, 'industry': 1, 'settle': 1})\n",
      "60\n",
      "[{'snippet': 722146, 'counts': 1}, {'snippet': 722149, 'counts': 4}, {'snippet': 722145, 'counts': 2}, {'snippet': 722148, 'counts': 2}]\n",
      "[722146, 722149, 722149, 722149, 722149, 722145, 722145, 722148, 722148]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"There is record of the French playwright Molière having attended many performances of the comédie italienne, or \\\"commedia dell\\'arte\\\". He is even referenced in a performance by Angelo Costantini of his show \\\"Une Vie de Scaramouche\\\", which refers to the writer and poet. This might suggest that the servant character in many of Molière\\'s plays, such as Dorine in his play \\\"Tartuffe\\\", might be based on this particular character archetype from the \\\"commedia dell'arte\\\".\"\n",
    "sentence = \"The City of New York, often called New York City or simply New York, is the most populous city in the United States.[9] With an estimated 2017 population of 8,622,698[7] distributed over a land area of about 302.6 square miles (784 km2),[10][11] New York City is also the most densely populated major city in the United States.[12] Located at the southern tip of the state of New York, the city is the center of the New York metropolitan area, one of the most populous urban agglomerations in the world,[13][14] with an estimated 20.3 million people in its 2017 Metropolitan Statistical Area and 23.7 million residents in its Combined Statistical Area.[4][5] A global power city,[15] New York City has been described as the cultural, financial, and media capital[16][17] of the world,[18][19][20][21][22] and exerts a significant impact upon commerce,[22] entertainment, research, technology, education, politics, tourism, and sports. The city's fast pace[23][24] defines the term New York minute.[25] Home to the headquarters of the United Nations,[26] New York is an important center for international diplomacy.\"\n",
    "sentence = \"My name is   André, Silva and I like to practice wind   Surf and   Albert Einstein the   Rockefeller University and New York City\"\n",
    "#sentence = 'The United States of America are a great country'\n",
    "#sentence = 'Apple is looking at buying U.K. startup for $1 billion'\n",
    "sentence = \"The cornell annual The budget for arXiv is approximately $826,000 for 2013 to 2017, funded jointly by Cornell University Library, the Simons Foundation (in both gift and challenge grant forms) and annual fee income from member institutions. This model arose in 2010, when Cornell sought to broaden the financial funding of the project by asking institutions to make annual voluntary contributions based on the amount of download usage by each institution. Annual donations were envisaged to vary in size between $2,300 to $4,000, based on each institution’s usage. , 174 institutions have pledged support for the period 2013–2017 on this basis, with a projected revenue from this source of approximately $340,000.\"\n",
    "\n",
    "#cleaned_texts = corp.clean_text(sentence, remove_stop_words=True, remove_punct=True, lemmas=True, remove_numbers=True, remove_spaces=True, remove_2letters_words=True, remove_apostrophe=True, method='spacy', spacy_disabled_components=['tagger', 'parser'])\n",
    "#print (cleaned_texts)\n",
    "\n",
    "\n",
    "\n",
    "data = get_cleaned_data()\n",
    "word_counts_per_snippet = corp.get_word_counts_per_snippet(data, clean_text=False)\n",
    "print (word_counts_per_snippet[5])\n",
    "freqs = corp.get_frequencies(word_counts_per_snippet)\n",
    "print (freqs['america'])\n",
    "\n",
    "snippets_by_word = corp.get_snippets_by_word(word_counts_per_snippet)\n",
    "print (snippets_by_word['basketball'])\n",
    "#print (snippets_by_word.keys())\n",
    "\n",
    "snippets_by_word2 = corp.get_snippets_by_word2(data)\n",
    "print (snippets_by_word2['basketball'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T13:37:41.589271Z",
     "start_time": "2018-03-27T13:37:14.207617Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(3520, 1000)\n",
      "sorting indices\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import safe_indexing\n",
    "\n",
    "# setup SOM dimensions\n",
    "H, W, N = 64, 64, 1000   # Network height, width and unit dimensions\n",
    "\n",
    "df_train = get_cleaned_data()\n",
    "#df_train = df_train.set_index(\"id\", drop = True)\n",
    "\n",
    "train_data = df_train.text\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "                            lowercase=False,\n",
    "                            #max_df=0.5, \n",
    "                            max_features=N,  \n",
    "                            stop_words='english', \n",
    "                            use_idf=True)\n",
    "X = vectorizer.fit_transform(train_data)\n",
    "\n",
    "#X = safe_indexing(X, df_train.index.values)\n",
    "#x = X.data \n",
    "#csr_matrix((X.data, df_train.index.values, indptr), shape=(3, 3)).toarray()\n",
    "\n",
    "print (type(X))\n",
    "print (X.shape)\n",
    "#print(vectorizer.get_feature_names())\n",
    "\n",
    "# setup SOM network\n",
    "som = Som(H, W, N, topology.RECT, verbose=True) # , verbose=True\n",
    "# reinit the codebook (not needed)\n",
    "som.codebook = np.random.rand(H, W, N).astype(som.codebook.dtype, copy=False)\n",
    "\n",
    "# train the SOM\n",
    "som.train(X)\n",
    "print (type(som.codebook))\n",
    "\n",
    "#serializing objects \n",
    "np.save('./codebook.npy', som.codebook)\n",
    "df_train.to_pickle('./dataframe.pkl')\n",
    "np.savez('./x.npz', data=X.data, indices=X.indices, indptr=X.indptr, shape=X.shape)\n",
    "\n",
    "\n",
    "#X[5]\n",
    "#print (df_train.text[578553])\n",
    "#bmus = som.bmus(X[1])\n",
    "#print (bmus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T00:19:08.065112Z",
     "start_time": "2018-03-27T00:18:38.231420Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########  continental  ########\n",
      "########  europe  ########\n",
      "########  term  ########\n",
      "########  school  ########\n"
     ]
    }
   ],
   "source": [
    "codebook = np.load('./codebook.npy')\n",
    "dataframe = pd.read_pickle('./dataframe.pkl')\n",
    "loader = np.load('./x.npz')\n",
    "\n",
    "X = csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'])\n",
    "\n",
    "som = Som(H, W, N, topology.RECT, verbose=True) # , verbose=True\n",
    "som.codebook = codebook\n",
    "\n",
    "word_counts_per_snippet = corp.get_word_counts_per_snippet(dataframe, clean_text=False)\n",
    "snippets_by_word = corp.get_snippets_by_word(word_counts_per_snippet)\n",
    "\n",
    "i = 0\n",
    "for key, value in snippets_by_word.items():\n",
    "    if i < 5:\n",
    "        if key != 'id':\n",
    "            a = np.zeros((H, W), dtype=np.int)\n",
    "            print ('########  ' +str(key)+ '  ########')\n",
    "            for snippet_count in value:\n",
    "                idx =  df_train.index[df_train['id'] == snippet_count['snippet']].tolist()[0]\n",
    "                bmus = som.bmus(X[idx])\n",
    "                #print (bmus[0].tolist())\n",
    "                a[bmus[0][0], bmus[0][1]] = snippet_count['counts']\n",
    "                \n",
    "            im = Image.fromarray(a)\n",
    "            im.save(\"./images/\"+key+\".png\")  \n",
    "    else:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T13:38:29.341800Z",
     "start_time": "2018-03-27T13:38:27.747736Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3520, 1000)\n",
      "########  cat  ########\n"
     ]
    }
   ],
   "source": [
    "codebook = np.load('./codebook.npy')\n",
    "dataframe = pd.read_pickle('./dataframe.pkl')\n",
    "loader = np.load('./x.npz')\n",
    "\n",
    "X = csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'])\n",
    "print (X.shape)\n",
    "word = 'cat'\n",
    "finger.create_fingerprint(word, dataframe, codebook, X, H, W, 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Export snippets to csv file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T11:37:54.260895Z",
     "start_time": "2018-04-05T11:37:52.070005Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_chunk_id = db.get_current_chunk_id(cfg)[0][0]\n",
    "chunck_size = 10000\n",
    "total_chunck_size = 55000\n",
    "\n",
    "chunck_size = 100\n",
    "total_chunck_size = 100\n",
    "\n",
    "\n",
    "engine = create_engine('postgresql://postgres@localhost:5432/sparsenlp')\n",
    "sql = \"select id, text from snippets where id >= \"+str(current_chunk_id)+\" and length(text) > 100 order by id limit \"+str(total_chunck_size)\n",
    "df = pd.read_sql_query(sql, con=engine)\n",
    "\n",
    "current = 0\n",
    "while current < total_chunck_size + 1:\n",
    "    \n",
    "    df_chunk = df[current: current + chunck_size]\n",
    "    df_chunk_min = df_chunk['id'].min()\n",
    "    df_chunk_max = df_chunk['id'].max()\n",
    "    #chunk_filename = 'raw_minid_maxid_size\n",
    "    chunk_filename = 'raw_'+str(df_chunk_min)+'_'+str(df_chunk_max)+'_'+str(df_chunk.shape[0])\n",
    "    df_chunk.to_csv('./chuncks/new/'+chunk_filename+'.bz2', index=False, compression='bz2', columns=['id', 'text'], encoding='utf-8')\n",
    "    #print (df_chunk.shape, df_chunk_min, df_chunk_max, chunk_filename)\n",
    "    #db.create_chunk(cfg, df_chunk_min, df_chunk_max, df_chunk.shape[0])\n",
    "    current += chunck_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T11:33:17.567029Z",
     "start_time": "2018-04-05T11:33:17.134058Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5779858"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
