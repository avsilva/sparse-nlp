{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T21:24:13.925877Z",
     "start_time": "2018-05-02T21:24:08.732942Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, json, sys, csv, shutil\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import datetime\n",
    "import utils.database as db\n",
    "import utils.corpora as corp\n",
    "import utils.fingerprints as finger\n",
    "\n",
    "import operator\n",
    "from scipy.sparse import csr_matrix\n",
    "from sparse_som import *\n",
    "import numpy as np\n",
    "import re\n",
    "import conf.conn as cfg\n",
    "#from tqdm import tnrange, tqdm_notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import safe_indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BaseFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T21:24:16.036128Z",
     "start_time": "2018-05-02T21:24:15.775820Z"
    },
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: <class 'FileNotFoundError'>\n"
     ]
    }
   ],
   "source": [
    "#set wikipedia folder where dump json files are located\n",
    "try:\n",
    "    wikifilesdir = \"F:/RESEARCH/TESE/corpora/wikifiles/01012018/json\"\n",
    "    os.chdir(wikifilesdir)\n",
    "except:\n",
    "    print('ERROR: ' +str(sys.exc_info()[0]))\n",
    "\n",
    "#register file system json files in DB\n",
    "def register():\n",
    "    i = 0\n",
    "    for subdir, dirs, files in os.walk(wikifilesdir):\n",
    "    \n",
    "        for file in files:\n",
    "            if i % 5 == 0:\n",
    "                print (str(i)+ ' files processed')\n",
    "            #print (os.path.join(subdir, file))\n",
    "            filepath = subdir + os.sep + file\n",
    "            path = filepath.split(os.sep)[-3:]\n",
    "            path = '/'.join(path)\n",
    "            db.register_file(cfg, path)\n",
    "            i = i + 1\n",
    "\n",
    "    print (\"DONE. %s files registered \"% (i))\n",
    "\n",
    "#process file system json files in DB and move file to processed destination folder\n",
    "def process():\n",
    "    limit = 100\n",
    "    i = 0\n",
    "    files = db.select_files(cfg, limit)\n",
    "    processed_dir = wikifilesdir+'-processed'\n",
    "    for file in files:\n",
    "        if i % 5 == 0:\n",
    "            print (str(i)+ ' files processed')\n",
    "        path = file[1]\n",
    "        file_id = file[0]\n",
    "        filepath = wikifilesdir+'/'+path\n",
    "        with open(filepath, 'r', encoding='utf-8') as fh:\n",
    "            datafile=fh.readlines()\n",
    "            db.process_file(cfg, datafile, file_id)\n",
    "        fh.close();\n",
    "        move_file(path, wikifilesdir, processed_dir)\n",
    "        i = i + 1\n",
    "\n",
    "\n",
    "def calculate_frequencies():\n",
    "    engine = create_engine('postgresql://postgres@localhost:5432/sparsenlp')\n",
    "    sql = \"select id, cleaned_text, bmu_x, bmu_y from snippets where cleaned = 't'\"\n",
    "    data = pd.read_sql_query(sql, con=engine)\n",
    "    word_counts_per_snippet = corp.get_word_counts_per_snippet(data)\n",
    "    freqs = corp.get_frequencies(word_counts_per_snippet)\n",
    "    vocabulary = corp.get_vocabulary(word_counts_per_snippet)\n",
    "    return data, vocabulary, freqs, word_counts_per_snippet\n",
    "        \n",
    "def update_stats():\n",
    "    data, vocabulary, freqs, word_counts_per_snippet = calculate_frequencies()\n",
    "    db.update_stats(cfg, data, vocabulary)\n",
    "    \n",
    "def process_snippets(_limit):\n",
    "    #tokenizer for snippets \n",
    "    #500 snippets -> 30 min\n",
    "    engine = create_engine('postgresql://postgres@localhost:5432/sparsenlp')\n",
    "    sql = \"select id, text from snippets\"\n",
    "    sql += \"where length(text) > 100 and cleaned = 'f' and random() < 0.01 limit \"+str(_limit)\n",
    "    data = pd.read_sql_query(sql, con=engine)\n",
    "    #print (data.shape)\n",
    "    cleaned_texts = corp.clean_text(data, remove_stop_words=True, remove_punct=True, \n",
    "                   lemmas=True, remove_numbers=True, remove_spaces=True, remove_2letters_words=True, \n",
    "                   remove_apostrophe=True, method='spacy', spacy_disabled_components=['tagger', 'parser'])\n",
    "    for text in cleaned_texts:\n",
    "        print (text['id'])\n",
    "        db.insert_cleaned_text(cfg, text['id'], text['snippet'])\n",
    "        \n",
    "def get_cleaned_data():\n",
    "    conn_string = 'postgresql://postgres@localhost:5432/sparsenlp'\n",
    "    engine = create_engine(conn_string)\n",
    "    sql = \"select id, cleaned_text from snippets where cleaned = 't'\"\n",
    "    dataframe = pd.read_sql_query(sql, con=engine)\n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T21:27:41.346736Z",
     "start_time": "2018-05-02T21:24:21.532137Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#register()\n",
    "#process()\n",
    "#get_tables_sizes()\n",
    "#process_snippets(10)\n",
    "#update_stats()\n",
    "\n",
    "#engine = create_engine('postgresql://postgres@localhost:5432/sparsenlp')\n",
    "#sql = \"select id, cleaned_text, bmu_x, bmu_y from snippets where cleaned = 't'\"\n",
    "#data = pd.read_sql_query(sql, con=engine)\n",
    "#word_counts_per_snippet = corp.get_word_counts_per_snippet(data)\n",
    "#print (word_counts_per_snippet[1])\n",
    "#snippets_by_word = corp.get_snippets_by_word(word_counts_per_snippet)\n",
    "\n",
    "#print (snippets_by_word['musician'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T21:09:01.109508Z",
     "start_time": "2018-04-22T20:59:51.367176Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data from  dataframe ...\n",
      "get data done\n",
      "vectorizing texts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizing texts done\n",
      "(305554, 1000)\n",
      "I'm training the SOM, so take a break and have some coffee ...\n",
      "sorting indices\n",
      "<class 'numpy.ndarray'>\n",
      "serializing files ...\n"
     ]
    }
   ],
   "source": [
    "def generate_som():\n",
    "    # setup SOM dimensions\n",
    "    # Network height, width and unit dimensions\n",
    "    H, W, N = 64, 64, 1000   \n",
    "    #H, W, N = 128, 128, 500\n",
    "\n",
    "    #som_type = 'SDSOM'\n",
    "    som_type = 'BSOM'\n",
    "\n",
    "    print (\"getting data from  dataframe ...\")\n",
    "    df_train = get_cleaned_data()\n",
    "    print (\"get data done\")\n",
    "\n",
    "    train_data = df_train.cleaned_text\n",
    "\n",
    "    print (\"vectorizing texts ...\")\n",
    "    vectorizer = TfidfVectorizer(\n",
    "                                lowercase=True,\n",
    "                                #max_df=0.5, \n",
    "                                max_features=N,  \n",
    "                                stop_words='english', \n",
    "                                use_idf=True)\n",
    "    X = vectorizer.fit_transform(train_data)\n",
    "    print (\"vectorizing texts done\")\n",
    "\n",
    "    sufix = '_'+som_type+'_'+str(H)+'_'+str(N)+'_'+str(X.shape[0])\n",
    "\n",
    "    #print (type(X))\n",
    "    print (X.shape)\n",
    "    #print(vectorizer.get_feature_names())\n",
    "    \n",
    "    # setup SOM network\n",
    "    #som = Som(H, W, N, topology.RECT, verbose=True) # , verbose=True\n",
    "    som = BSom(H, W, N, topology.RECT, verbose=True) # , verbose=True\n",
    "\n",
    "\n",
    "    # reinit the codebook (not needed) #try to use minisom to init the codebook based on data\n",
    "    som.codebook = np.random.rand(H, W, N).astype(som.codebook.dtype, copy=False)\n",
    "\n",
    "    print (\"I'm training the SOM, so take a break, relax and have some coffee ...\")\n",
    "    time1 = datetime.datetime.now()\n",
    "    som.train(X)\n",
    "    time2 = datetime.datetime.now()\n",
    "    elapsedTime = time2 - time1\n",
    "    minutes = divmod(elapsedTime.total_seconds(), 60)[0]\n",
    "\n",
    "    db.insert_log(cfg, som_type+' trainning', sufix, minutes)\n",
    "    print (type(som.codebook))\n",
    "\n",
    "\n",
    "    print (\"serializing files ...\")\n",
    "    np.save('./serializations/codebook'+sufix+'.npy', som.codebook)\n",
    "    df_train.to_pickle('./serializations/dataframe'+sufix+'.pkl')\n",
    "    np.savez('./serializations/X'+sufix+'.npz', data=X.data, indices=X.indices, indptr=X.indptr, shape=X.shape)\n",
    "\n",
    "    engine = create_engine('postgresql://postgres@localhost:5432/sparsenlp')\n",
    "    #sql = \"select id, cleaned_text, bmu_x, bmu_y from snippets where cleaned = 't'\"\n",
    "    sql = \"select id, cleaned_text from snippets where cleaned = 't'\"\n",
    "    data = pd.read_sql_query(sql, con=engine)\n",
    "    word_counts_per_snippet = corp.get_word_counts_per_snippet(data)\n",
    "    #\n",
    "    with open('./serializations/word_counts_per_snippet'+sufix+'.pkl', 'wb') as f:\n",
    "        pickle.dump(word_counts_per_snippet, f)\n",
    "    #\n",
    "    snippets_by_word = corp.get_snippets_by_word(word_counts_per_snippet)\n",
    "    with open('./serializations/snippets_by_word'+sufix+'.pkl', 'wb') as f:\n",
    "        pickle.dump(snippets_by_word, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-22T21:15:44.795729Z",
     "start_time": "2018-04-22T21:11:43.965333Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loading snippets_by_word_BSOM_64_1000_305554\n",
      "./serializations/snippets_by_word_BSOM_64_1000_305554 load done\n",
      "(305554, 1000)\n",
      "changing codebook type to float64\n",
      "########  dog  ########\n",
      "(array([3464,  443,  111,   41,   16,   11,    2,    4,    2,    2],\n",
      "      dtype=int64), array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]))\n"
     ]
    }
   ],
   "source": [
    "def create_fingerprint():\n",
    "    H, W, N, rows = 64, 64, 1000, 305554    # Network height, width and unit dimensions\n",
    "    som_type = 'BSOM'\n",
    "    sufix = '_'+som_type+'_'+str(H)+'_'+str(N)+'_'+str(rows)\n",
    "\n",
    "\n",
    "    codebook = np.load('./serializations/codebook'+sufix+'.npy')\n",
    "    dataframe = pd.read_pickle('./serializations/dataframe'+sufix+'.pkl')\n",
    "    loader = np.load('./serializations/X'+sufix+'.npz')\n",
    "    print (' loading snippets_by_word'+sufix)   \n",
    "    with open('./serializations/snippets_by_word'+sufix+'.pkl', 'rb') as handle:\n",
    "        snippets_by_word = pickle.load(handle)\n",
    "    print ('./serializations/snippets_by_word'+sufix+' load done')\n",
    "    X = csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'])\n",
    "    print (X.shape)\n",
    "\n",
    "    word = 'dog'\n",
    "    time1 = datetime.datetime.now()\n",
    "    a_original, a_sparse = finger.create_fingerprint(word, dataframe, snippets_by_word, codebook, X, H, W, sufix)\n",
    "    time2 = datetime.datetime.now()\n",
    "    elapsedTime = time2 - time1\n",
    "    minutes = divmod(elapsedTime.total_seconds(), 60)[0]\n",
    "    db.insert_log(cfg, 'create fingerprint', sufix, minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T16:13:03.484836Z",
     "start_time": "2018-05-01T16:13:03.480829Z"
    }
   },
   "source": [
    "#### Generate Image from Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T16:12:36.935786Z",
     "start_time": "2018-05-01T16:12:36.105372Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdgAAAHVCAYAAABSR+pHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC88zLlI+U1d8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fa9a44940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "file = './images/musician_BSOM_64_1000_305554.bmp'\n",
    "\n",
    "image = ndimage.imread(file) \n",
    "plt.figure(figsize = (15,8))\n",
    "plt.imshow(image)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Export chunks of snippets to csv files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-09T10:17:13.883095Z",
     "start_time": "2018-04-09T10:15:18.391534Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2) 780465 791784 32_780465_791784_10000\n",
      "(10000, 2) 791785 803151 33_791785_803151_10000\n",
      "(10000, 2) 803152 814309 34_803152_814309_10000\n",
      "(10000, 2) 814310 825687 35_814310_825687_10000\n",
      "(10000, 2) 825688 841072 36_825688_841072_10000\n",
      "(10000, 2) 841077 853301 37_841077_853301_10000\n",
      "(10000, 2) 853302 864829 38_853302_864829_10000\n",
      "(10000, 2) 864832 876839 39_864832_876839_10000\n",
      "(10000, 2) 876840 888780 40_876840_888780_10000\n",
      "(10000, 2) 888781 900947 41_888781_900947_10000\n",
      "(10000, 2) 900948 914225 42_900948_914225_10000\n",
      "(10000, 2) 914226 926054 43_914226_926054_10000\n",
      "(10000, 2) 926055 937677 44_926055_937677_10000\n",
      "(10000, 2) 937678 949258 45_937678_949258_10000\n",
      "(10000, 2) 949259 961087 46_949259_961087_10000\n",
      "(10000, 2) 961088 972730 47_961088_972730_10000\n",
      "(10000, 2) 972731 984446 48_972731_984446_10000\n",
      "(10000, 2) 984447 996127 49_984447_996127_10000\n",
      "(10000, 2) 996128 1007748 50_996128_1007748_10000\n",
      "(10000, 2) 1007749 1019442 51_1007749_1019442_10000\n"
     ]
    }
   ],
   "source": [
    "def create_chunk(chunck_size, total_chunck_size):\n",
    "    current_chunk_id = db.get_current_chunk_id(cfg)[0][0]\n",
    "    engine = create_engine('postgresql://postgres@localhost:5432/sparsenlp')\n",
    "    sql = \"select id, text from snippets where id >= \"+str(current_chunk_id)+\" and length(text) > 100 order by id limit \"+str(total_chunck_size)\n",
    "    df = pd.read_sql_query(sql, con=engine)\n",
    "\n",
    "    current = 0\n",
    "    while current < total_chunck_size + 1:\n",
    "\n",
    "        df_chunk = df[current: current + chunck_size]\n",
    "        df_chunk_min = df_chunk['id'].min()\n",
    "        df_chunk_max = df_chunk['id'].max()\n",
    "        if df_chunk.shape[0] > 0:\n",
    "            id = db.create_chunk(cfg, df_chunk_min, df_chunk_max, df_chunk.shape[0])\n",
    "            chunk_filename = str(id)+'_'+str(df_chunk_min)+'_'+str(df_chunk_max)+'_'+str(df_chunk.shape[0])\n",
    "            df_chunk.to_csv('./chuncks/new/'+chunk_filename+'.bz2', index=False, compression='bz2', columns=['id', 'text'], encoding='utf-8')\n",
    "            print (df_chunk.shape, df_chunk_min, df_chunk_max, chunk_filename)\n",
    "\n",
    "        current += chunck_size\n",
    "\n",
    "#create_chunk(10, 20)\n",
    "create_chunk(10000, 200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Update database with tokens from chunk files</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T14:55:50.858347Z",
     "start_time": "2018-04-21T14:54:19.935029Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSING FILE: C:/AVS/ACADEMIC/ISCTE/TESE/andre/sparse-nlp/chuncks/processed/32_780465_791784_10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def update_tokens(_file=None):\n",
    "    try:\n",
    "        chunkfilesdir = \"C:/AVS/ACADEMIC/ISCTE/TESE/andre/sparse-nlp/chuncks\"\n",
    "    except:\n",
    "        print('ERROR: ' +str(sys.exc_info()[0]))\n",
    "        \n",
    "    if  _file is not None:\n",
    "        \n",
    "        if os.path.isfile(chunkfilesdir+'/processed/'+_file):\n",
    "            print ('PROCESSING FILE: '+chunkfilesdir+'/processed/'+_file)\n",
    "            with open(chunkfilesdir+'/processed/'+_file, 'r', encoding='utf-8') as f:\n",
    "                reader = csv.reader(f)\n",
    "                rownum = 0\n",
    "                db.batch_update_cleaned_text_file(cfg, reader, _file)\n",
    "            shutil.move(chunkfilesdir+'/processed/'+_file, chunkfilesdir+'/done/'+_file)\n",
    "            update_stats()\n",
    "        else:\n",
    "            print ('FILE DOES NOT EXIST')\n",
    "    else:\n",
    "        for dirpath, dirnames, filenames in os.walk(chunkfilesdir+'/processed/'):\n",
    "            allfiles = filenames\n",
    "        for filename in allfiles:\n",
    "            update_tokens(filename)\n",
    "            \n",
    "\n",
    "update_tokens('32_780465_791784_10000')\n",
    "#update_tokens()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T16:54:03.736227Z",
     "start_time": "2018-04-21T16:52:46.689343Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12943\n",
      "------\n",
      "[('id', 187689597180), ('the', 206265), ('include', 47477), ('time', 43436), ('state', 36749)]\n",
      "------\n",
      "Counter({'id': 608678, 'oil': 4, 'field': 2, 'divest': 2, 'interest': 2, 'the': 1, 'niger': 1, 'delta': 1, 'nembe': 1, 'creek': 1, 'discover': 1, 'produce': 1, 'middle': 1, 'miocene': 1, 'deltaic': 1, 'sandstone': 1, 'shale': 1, 'anticline': 1, 'structural': 1, 'trap': 1, 'depth': 1, 'shell': 1, 'announce': 1, 'strategic': 1, 'review': 1, 'operation': 1, 'nigeria': 1, 'hint': 1, 'asset': 1, 'while': 1, 'international': 1, 'company': 1, 'operate': 1, 'decades': 1, 'make': 1, 'move': 1, 'cite': 1, 'range': 1, 'issue': 1, 'include': 1, 'theft': 1, 'shell_oil_company': 1, 'say': 1, 'finalise': 1, 'nigerian': 1})\n",
      "------\n",
      "O vocabulário tem 262168 palavras\n"
     ]
    }
   ],
   "source": [
    "data, vocabulary, freqs, word_counts_per_snippet = calculate_frequencies()\n",
    "\n",
    "print (freqs['war'])\n",
    "print ('------')\n",
    "print (sorted(freqs.items(), key=operator.itemgetter(1), reverse=True)[:5])\n",
    "print ('------')\n",
    "print (word_counts_per_snippet[5])\n",
    "print ('------')\n",
    "print(\"O vocabulário tem %d palavras\"% len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_train = get_cleaned_data()\n",
    "print (len(snippets_by_word['sunlight']))\n",
    "#print (snippets_by_word['sunlight'][0:3])\n",
    "#print (df_train.iloc[2497]['cleaned_text'])\n",
    "\n",
    "H, W, N, rows = 64, 64, 1000, 305554    # Network height, width and unit dimensions\n",
    "som_type = 'BSOM'\n",
    "sufix = '_'+som_type+'_'+str(H)+'_'+str(N)+'_'+str(rows)\n",
    "codebook = np.load('./serializations/codebook'+sufix+'.npy')\n",
    "dataframe = pd.read_pickle('./serializations/dataframe'+sufix+'.pkl')\n",
    "loader = np.load('./serializations/X'+sufix+'.npz')\n",
    "print (' loading snippets_by_word'+sufix)   \n",
    "\n",
    "som = Som(H, W, N, topology.RECT, verbose=True) # , verbose=True\n",
    "som.codebook = codebook\n",
    "\n",
    "#one possibility to speed up fingerprint generation is to pre-calculate bmus for each snippet (and save it to table)\n",
    "#for idx, row in df_train.iterrows():\n",
    "#    bmus = som.bmus(X[idx])\n",
    "#    print (idx, bmus)\n",
    "#    if idx > 100:\n",
    "#        sys.exit(0)\n",
    "    #idx = snippet_count['idx']\n",
    "    #bmus = som.bmus(X[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
